{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bead2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import tiktoken\n",
    "import spacy\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load spaCy for sentence splitting\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load OpenAI tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # Use the appropriate tokenizer for your LLM (e.g., GPT-3, GPT-4).\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a given text using the OpenAI tokenizer.\n",
    "    \"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "\n",
    "def split_sentences_spacy(text):\n",
    "    \"\"\"\n",
    "    Split the input text into sentences using spaCy's sentence boundary detection.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "\n",
    "def split_long_sentence(sentence, max_tokens=500):\n",
    "    \"\"\"\n",
    "    Safely split a very long sentence into smaller chunks using the OpenAI tokenizer.\n",
    "    Ensures no chunk exceeds the maximum token limit.\n",
    "    \"\"\"\n",
    "    token_ids = tokenizer.encode(sentence)\n",
    "    chunks = []\n",
    "    for i in range(0, len(token_ids), max_tokens):\n",
    "        token_chunk = tokenizer.decode(token_ids[i:i + max_tokens])\n",
    "        chunks.append(token_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunk_text(text, max_tokens=500):\n",
    "    \"\"\"\n",
    "    Chunk the input text into smaller pieces using token limits.\n",
    "    Combines sentences into chunks while respecting the token limit.\n",
    "    \"\"\"\n",
    "    sentences = split_sentences_spacy(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        sent_tokens = count_tokens(sent)\n",
    "\n",
    "        # Handle sentences that exceed the max token limit\n",
    "        if sent_tokens > max_tokens:\n",
    "            sub_chunks = split_long_sentence(sent, max_tokens)\n",
    "            chunks.extend(sub_chunks)\n",
    "            continue\n",
    "\n",
    "        # Add sentences to the current chunk without exceeding the token limit\n",
    "        if current_tokens + sent_tokens <= max_tokens:\n",
    "            current_chunk += \" \" + sent\n",
    "            current_tokens += sent_tokens\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sent\n",
    "            current_tokens = sent_tokens\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def extract_pdf_content(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text content from a PDF document using PyMuPDF.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    extracted_content = []\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text()  # Extract text from the current page\n",
    "\n",
    "        if text.strip():  # Skip empty pages\n",
    "            extracted_content.append({\n",
    "                \"page_number\": page_num + 1,\n",
    "                \"text\": text\n",
    "            })\n",
    "\n",
    "    return extracted_content\n",
    "\n",
    "\n",
    "def process_pdf_for_training(pdf_path, output_dir=\"output_data\", max_tokens=500):\n",
    "    \"\"\"\n",
    "    Extract, chunk, and tokenize text from a PDF for LLM training.\n",
    "    Saves the result in JSON format for further use.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Extract text from the PDF\n",
    "    pdf_content = extract_pdf_content(pdf_path)\n",
    "\n",
    "    # Initialize processed data for training\n",
    "    training_data = []\n",
    "\n",
    "    for page in pdf_content:\n",
    "        page_number = page[\"page_number\"]\n",
    "        text = page[\"text\"]\n",
    "\n",
    "        # Chunk and tokenize the text\n",
    "        chunks = chunk_text(text, max_tokens)\n",
    "\n",
    "        # Append each chunk as training data\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            training_data.append({\n",
    "                \"page_number\": page_number,\n",
    "                \"chunk_number\": idx + 1,\n",
    "                \"chunk_text\": chunk\n",
    "            })\n",
    "\n",
    "    # Save the processed data to a JSON file\n",
    "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    output_path = os.path.join(output_dir, f\"{pdf_name}_training_data.json\")\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(training_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Processed data saved to {output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    input_pdf = \"example.pdf\"  # Replace with the path to your PDF file\n",
    "    process_pdf_for_training(input_pdf, max_tokens=500)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
